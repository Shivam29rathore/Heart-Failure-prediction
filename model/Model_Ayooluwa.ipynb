{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series, DataFrame\n",
    "import sklearn.utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, LeaveOneOut, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score, classification_report, accuracy_score, precision_score, make_scorer, f1_score, confusion_matrix \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import  ColumnTransformer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0  75.0        0                       582         0                 20   \n",
       "1  55.0        0                      7861         0                 38   \n",
       "2  65.0        0                       146         0                 20   \n",
       "3  50.0        1                       111         0                 20   \n",
       "4  65.0        1                       160         1                 20   \n",
       "\n",
       "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                    1  265000.00               1.9           130    1   \n",
       "1                    0  263358.03               1.1           136    1   \n",
       "2                    0  162000.00               1.3           129    1   \n",
       "3                    0  210000.00               1.9           137    1   \n",
       "4                    0  327000.00               2.7           116    0   \n",
       "\n",
       "   smoking  time  DEATH_EVENT  \n",
       "0        0     4            1  \n",
       "1        0     6            1  \n",
       "2        1     7            1  \n",
       "3        0     7            1  \n",
       "4        0     8            1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dataset\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/gogzicole/stage-f-07-heart-failure/master/data/heart_failure_clinical_records_dataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    203\n",
       "1     96\n",
       "Name: DEATH_EVENT, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# selecting feature matrix and target variable\n",
    "X = dataset.drop(columns = 'DEATH_EVENT')\n",
    "y = dataset['DEATH_EVENT']\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "make_column_transformer = [2,4,6,7,8,11] #columns to apply min max scaling\n",
    "#Create set of hyperparameters to search through randomly\n",
    "n_estimators = [50, 100, 300, 500, 1000] \n",
    "\n",
    "min_samples_split = [2, 3, 5, 7, 9]\n",
    "\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "max_features = ['auto', 'sqrt', 'log2', None] \n",
    "\n",
    "hyperparameter_grid = {'n_estimators': n_estimators,\n",
    "\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "                       'min_samples_split': min_samples_split,\n",
    "\n",
    "                       'max_features': max_features}\n",
    "#Integrate everything into a pipeline\n",
    "pipe = Pipeline([('col_transform', ColumnTransformer(remainder = 'passthrough',\n",
    "                transformers = [('scaler',MinMaxScaler(),make_column_transformer)])),\n",
    "                ('random_search',RandomizedSearchCV(ExtraTreesClassifier(random_state = 1),\n",
    "                hyperparameter_grid, scoring = 'f1', verbose = 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1 = pipe.named_steps['col_transform'].fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test,y_train, y_test = train_test_split(step_1, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   14.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('col_transform',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('scaler', MinMaxScaler(),\n",
       "                                                  [2, 4, 6, 7, 8, 11])])),\n",
       "                ('random_search',\n",
       "                 RandomizedSearchCV(estimator=ExtraTreesClassifier(random_state=1),\n",
       "                                    param_distributions={'max_features': ['auto',\n",
       "                                                                          'sqrt',\n",
       "                                                                          'log2',\n",
       "                                                                          None],\n",
       "                                                         'min_samples_leaf': [1,\n",
       "                                                                              2,\n",
       "                                                                              4,\n",
       "                                                                              6,\n",
       "                                                                              8],\n",
       "                                                         'min_samples_split': [2,\n",
       "                                                                               3,\n",
       "                                                                               5,\n",
       "                                                                               7,\n",
       "                                                                               9],\n",
       "                                                         'n_estimators': [50,\n",
       "                                                                          100,\n",
       "                                                                          300,\n",
       "                                                                          500,\n",
       "                                                                          1000]},\n",
       "                                    scoring='f1', verbose=1))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(max_features=None, min_samples_leaf=2, min_samples_split=7,\n",
       "                     n_estimators=1000, random_state=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = pipe.named_steps['random_search'].best_estimator_\n",
    "model1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred1 = model1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification report for the Dataset as is with Extra trees classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88        37\n",
      "           1       0.93      0.61      0.74        23\n",
      "\n",
      "    accuracy                           0.83        60\n",
      "   macro avg       0.87      0.79      0.81        60\n",
      "weighted avg       0.85      0.83      0.82        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report1 = classification_report(y_test,y_pred1)\n",
    "print('The classification report for the Dataset as is with Extra trees classifier:')\n",
    "print(report1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsample\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=1)\n",
    "X_balanced, y_balanced = smote.fit_sample(X, y)\n",
    "X_balanced = DataFrame(X_balanced, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   27.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('col_transform',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('scaler', MinMaxScaler(),\n",
       "                                                  [2, 4, 6, 7, 8, 11])])),\n",
       "                ('random_search',\n",
       "                 RandomizedSearchCV(estimator=ExtraTreesClassifier(random_state=1),\n",
       "                                    param_distributions={'max_features': ['auto',\n",
       "                                                                          'sqrt',\n",
       "                                                                          'log2',\n",
       "                                                                          None],\n",
       "                                                         'min_samples_leaf': [1,\n",
       "                                                                              2,\n",
       "                                                                              4,\n",
       "                                                                              6,\n",
       "                                                                              8],\n",
       "                                                         'min_samples_split': [2,\n",
       "                                                                               3,\n",
       "                                                                               5,\n",
       "                                                                               7,\n",
       "                                                                               9],\n",
       "                                                         'n_estimators': [50,\n",
       "                                                                          100,\n",
       "                                                                          300,\n",
       "                                                                          500,\n",
       "                                                                          1000]},\n",
       "                                    scoring='f1', verbose=1))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "step_bal = pipe.named_steps['col_transform'].fit_transform(X_balanced)\n",
    "x_train, x_test,y_train, y_test = train_test_split(step_bal, y_balanced, test_size = 0.2, random_state = 0)\n",
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "model_bal = pipe.named_steps['random_search'].best_estimator_\n",
    "model_bal.fit(x_train, y_train)\n",
    "y_pred_bal = model_bal.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Recall for smoted Extra Trees with upsampling is:0.9149\n",
      "the Accuracy for smoted Extra Trees with upsampling is:0.878\n",
      "the Precision for smoted Extra Trees with upsampling  is: 0.8776\n",
      "the F1_score for smoted Extra Trees with upsampling  is: 0.8958\n"
     ]
    }
   ],
   "source": [
    "#metrics\n",
    "recall_bal = recall_score(y_test,y_pred_bal)\n",
    "accuracy_bal = accuracy_score(y_test,y_pred_bal)\n",
    "precision_bal = precision_score(y_test,y_pred_bal)\n",
    "f1score_bal = f1_score(y_test,y_pred_bal)\n",
    "print('the Recall for smoted Extra Trees with upsampling is:{}'.format(round(recall_bal,4)))\n",
    "print(f'the Accuracy for smoted Extra Trees with upsampling is:{round(accuracy_bal,4)}')\n",
    "print('the Precision for smoted Extra Trees with upsampling  is: %s' %(round(precision_bal,4)))\n",
    "print(f'the F1_score for smoted Extra Trees with upsampling  is: {round(f1score_bal,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build new pipeline with XGB classifier\n",
    "pipe_XG = Pipeline([('col_transform', ColumnTransformer(remainder = 'passthrough',\n",
    "                transformers = [('scaler',MinMaxScaler(),make_column_transformer)])),\n",
    "                ('random_search',RandomizedSearchCV(XGBClassifier(random_state = 1),\n",
    "                hyperparameter_grid, scoring = 'f1', verbose = 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('col_transform',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('scaler', MinMaxScaler(),\n",
       "                                                  [2, 4, 6, 7, 8, 11])])),\n",
       "                ('random_search',\n",
       "                 RandomizedSearchCV(estimator=XGBClassifier(random_state=1),\n",
       "                                    param_distributions={'max_features': ['auto',\n",
       "                                                                          'sqrt',\n",
       "                                                                          'log2',\n",
       "                                                                          None],\n",
       "                                                         'min_samples_leaf': [1,\n",
       "                                                                              2,\n",
       "                                                                              4,\n",
       "                                                                              6,\n",
       "                                                                              8],\n",
       "                                                         'min_samples_split': [2,\n",
       "                                                                               3,\n",
       "                                                                               5,\n",
       "                                                                               7,\n",
       "                                                                               9],\n",
       "                                                         'n_estimators': [50,\n",
       "                                                                          100,\n",
       "                                                                          300,\n",
       "                                                                          500,\n",
       "                                                                          1000]},\n",
       "                                    scoring='f1', verbose=1))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit upsampled data to XGB pipeline\n",
    "step_XG = pipe_XG.named_steps['col_transform'].fit_transform(X_balanced)\n",
    "x_train, x_test,y_train, y_test = train_test_split(step_XG, y_balanced, test_size = 0.2, random_state = 0)\n",
    "pipe_XG.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "model_XG = pipe_XG.named_steps['random_search'].best_estimator_\n",
    "model_XG.fit(x_train, y_train)\n",
    "y_pred_XG = model_XG.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Recall for smoted XGB is:0.9362\n",
      "the Accuracy for smoted XGB is:0.8902\n",
      "the Precision for smoted XGB is: 0.88\n",
      "the F1_score for smoted XGB is: 0.9072\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "recall_XG = recall_score(y_test,y_pred_XG)\n",
    "accuracy_XG = accuracy_score(y_test,y_pred_XG)\n",
    "precision_XG = precision_score(y_test,y_pred_XG)\n",
    "f1score_XG = f1_score(y_test,y_pred_XG)\n",
    "print('the Recall for smoted XGB is:{}'.format(round(recall_XG,4)))\n",
    "print(f'the Accuracy for smoted XGB is:{round(accuracy_XG,4)}')\n",
    "print('the Precision for smoted XGB is: %s' %(round(precision_XG,4)))\n",
    "print(f'the F1_score for smoted XGB is: {round(f1score_XG,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
